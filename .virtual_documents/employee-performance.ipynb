import numpy as np 
import pandas as pd 
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix , classification_report , f1_score , precision_score , recall_score 
from sklearn.model_selection import train_test_split , cross_val_score , RandomizedSearchCV , GridSearchCV


df = pd.read_csv('./archive-3/train_dataset.csv')
df.head()


df.describe()


df.info()





df.shape


df.isna().sum()


df['wip'].fillna(df['wip'].mean(),inplace = True)


df.isna().sum()


df['actual_productivity'].plot(kind = 'hist')


df.corr()


import seaborn as sns


corr_matrix = df.corr()
fig , ax = plt.subplots(figsize = (15 , 10))
sns.heatmap(corr_matrix,
linewidths=0.5)


X = df.drop(columns=['actual_productivity','target_productivity'], axis= 1)
y = df['actual_productivity']


X.columns


y


X_train , X_test , y_train , y_test = train_test_split(X , y , test_size = 0.2)


from sklearn.linear_model import SGDRegressor


model = SGDRegressor(random_state = 42)
model.fit(X_train,y_train)


model.score(X_test , y_test)


model.coef_


feature_dict = dict(zip(df.columns ,list(model.coef_)))
feature_dict


feature_df = pd.DataFrame(feature_dict , index=[0])
feature_df


feature_df.T.plot.bar(title='Feature Importance', legend = False)


from sklearn.linear_model import LinearRegression
reg = LinearRegression()
reg.fit(X_train , y_train)
reg.score(X_test , y_test)


from sklearn.ensemble import RandomForestRegressor
model = RandomForestRegressor()
model.fit(X_train , y_train)
model.score(X_test , y_test)


from sklearn.ensemble import GradientBoostingRegressor
model = GradientBoostingRegressor()
model.fit(X_train,y_train)
model.score(X_test,y_test)


from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import GridSearchCV

# Define the parameter grid
# Define the corrected parameter grid
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 5],
    'max_features': ['sqrt', 'log2'],  # Corrected values for max_features
    'bootstrap': [True, False]
}


# Create the Random Forest Regressor
rf_regressor = RandomForestRegressor(random_state=42)

# Instantiate GridSearchCV
grid_search = GridSearchCV(estimator=rf_regressor, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)

# Fit the grid search to the data
grid_search.fit(X_train, y_train)

# Get the best parameters
best_params = grid_search.best_params_



best_params


grid_search.score(X_test , y_test)


from sklearn.ensemble import StackingRegressor
from sklearn.linear_model import LinearRegression

# Create a stack of models
stack_model = StackingRegressor([
    ('rf', RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)),
    ('lr', LinearRegression())
], final_estimator=LinearRegression())

# Fit the stack model to the data
stack_model.fit(X_train, y_train)

# Evaluate the stack model
stack_model_score = stack_model.score(X_test, y_test)



stack_model_score


from sklearn.ensemble import StackingRegressor
from sklearn.linear_model import LinearRegression

# Create a stack of models
stack_model = StackingRegressor([
    ('rf', RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)),
    ('lr', LinearRegression())
], final_estimator=LinearRegression())

# Fit the stack model to the data
stack_model.fit(X, y)





from joblib import Parallel, delayed 
import joblib 
  
  
# Save the model as a pickle in a file 
joblib.dump(stack_model, 'stack_model.pkl') 



df = pd.read_csv('./archive-3/test_dataset.csv')
df.drop(columns='targeted_productivity',axis=1,inplace = True)
df.isna().sum()



df['wip'] = df['wip'].fillna(df['wip'].mean())


df.columns


loaded_stack_model = joblib.load('stack_model.pkl')


predictions = loaded_stack_model.predict(df)


import matplotlib.pyplot as plt

# Plotting the predictions
plt.figure(figsize=(10, 6))

# Histogram of Predictions
plt.subplot(221)
plt.hist(predictions, bins=20, color='skyblue', edgecolor='black')
plt.title('Distribution of Predictions')
plt.xlabel('Predicted Values')
plt.ylabel('Frequency')

# Scatter Plot of Predicted vs Index
plt.subplot(222)
plt.scatter(range(len(predictions)), predictions, color='orange', alpha=0.7)
plt.title('Predicted Values vs Index')
plt.xlabel('Index')
plt.ylabel('Predicted Values')

# Line Plot of Predicted Values
plt.subplot(223)
plt.plot(predictions, color='green')
plt.title('Trend of Predicted Values')
plt.xlabel('Index')
plt.ylabel('Predicted Values')

# Boxplot of Predictions
plt.subplot(224)
plt.boxplot(predictions, vert=False)
plt.title('Spread of Predictions')
plt.xlabel('Predicted Values')

# Adjust layout and display the plots
plt.tight_layout()
plt.show()



df.head()


import matplotlib.pyplot as plt
import seaborn as sns


# Combine features and predictions into a single DataFrame
data = pd.concat([df, pd.Series(predictions, name='Predictions')], axis=1)

# Plotting the variation in productivity with respect to different features
plt.figure(figsize=(12, 8))

# Example: Variation in productivity with respect to 'smv' (Standard Minute Value)
plt.subplot(231)
sns.scatterplot(x='smv', y='Predictions', data=data)
plt.title('Variation with SMV')
plt.ylabel('Productivity')
plt.xlabel('Standard Minute value')
# Example: Variation in productivity with respect to 'over_time'
plt.subplot(232)
sns.scatterplot(x='over_time', y='Predictions', data=data)
plt.title('Variation with Overtime')
plt.ylabel('Productivity')
plt.xlabel('Overtime')

plt.subplot(233)
sns.scatterplot(x='incentive', y='Predictions', data=data)
plt.title('Variation with Incentive')
plt.ylabel('Productivity')
plt.xlabel('Incentive')




# Adjust layout and display the plots
plt.tight_layout()
plt.show()




